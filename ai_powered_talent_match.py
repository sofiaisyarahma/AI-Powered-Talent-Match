# -*- coding: utf-8 -*-
"""AI-Powered Talent Match.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j5dkjF04hn3CkkMlmjIYOzpZhmQEeu83

# AI-Powered Talent Match


### Author:
**[Sofi Aisyarahma](https://www.linkedin.com/in/sofiaisyarahma/)**

### Description:
Company X is developing a Talent Match Intelligence system to help leaders identify what makes top-performing employees
successful and to find individuals who share those characteristics for succession.

### Outline:
1. Data Extraction
2. Data Cleaning
3. Exploratory Data Analysis
4. Machine Learning Model
5. Insights and Business Recommendations

### Objective:
1. Discover the pattern of success
    - Use data exploration and visualization to see how high performers differ from others.
    - Compare across:
       + Competency pillars (competencies_yearly, dim_competency_pillars)
       + Psychometric data (papi_scores, profiles_psych)
       + Behavioral data (strengths)
       + Contextual data (grade, education, years_of_service_months)
    - Outcome → a “Success Formula” that explains what drives high performance.
2. Operationalize the logic in SQL
    - Managers choose “benchmark employees” (rating = 5).
    - Write a SQL script that:
        + Calculates how closely every employee matches the benchmark.
        + Uses CTEs (Common Table Expressions) for clarity.
        + Produces per-variable, per-group, and final match % scores.
3. Build an AI-powered dashboard/app
     - Use tools like Streamlit or similar.
     - Show:
       + Ranked candidate list by match %.
       + Visuals (radar, heatmap, comparison charts).
       + AI-generated job profile and key competencies.
     - The app must take new input (role, benchmark employees) and recompute dynamically.


### Dataset: [Study Case DA](https://docs.google.com/spreadsheets/d/1WEEjTL02EhUvZtyhQ3xshYrvKa-d6900GzIqXLZzU1o/)

## Import Libraries
"""

# Import pandas library for data manipulation and analysis
import pandas as pd

# Import gdown library for downloading files from Google Drive (if needed)
import gdown

# Import numpy library for scientific computing and array manipulation
import numpy as np

# Import datetime library for working with dates and times
from datetime import datetime

# Import matplotlib.pyplot for creating visualizations (plots)
import matplotlib.pyplot as plt

# Import seaborn library for creating statistical data visualizations
import seaborn as sns

# Import warnings library to suppress warnings (use with caution)
import warnings
warnings.filterwarnings("ignore")  # **Caution:** May hide important warnings

# Set pandas display options to show all columns in the DataFrame (might be inconvenient for large datasets)
pd.set_option('display.max_columns', None)

# Import libraries from scipy.stats for statistical tests
from scipy.stats import shapiro, skew

plt.rcParams['axes.grid'] = False

# Install and import dependencies
!pip install --quiet gspread gspread_dataframe

# impoort gspread_dataframe library
import gspread
from gspread_dataframe import get_as_dataframe

# Authorize access to Google Sheets
from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default

creds, _ = default()
gc = gspread.authorize(creds)

"""## 1. Data Extraction

### 1.1 Import Dataset (GoogleSheets)
"""

# Open Google Sheets file by URL
spreadsheet = gc.open_by_url("https://docs.google.com/spreadsheets/d/1WEEjTL02EhUvZtyhQ3xshYrvKa-d6900GzIqXLZzU1o/edit?usp=sharing")

# List all available sheets
worksheet_list = spreadsheet.worksheets()
for ws in worksheet_list:
    print(ws.title)

# Load all sheets automatically
# Create a dictionary to store all sheets as pandas DataFrames
dfs = {}

for ws in spreadsheet.worksheets():
    title = ws.title
    df = get_as_dataframe(ws)
    dfs[title] = df

# Check which DataFrames you have
dfs.keys()

# Assign each sheet to a dataframe variable
tv_tgv              = dfs['Talent Variable (TV) & Talent Group Variable (TGV)']
dim_companies       = dfs['dim_companies']
dim_areas           = dfs['dim_areas']
dim_positions       = dfs['dim_positions']
dim_departments     = dfs['dim_departments']
dim_divisions       = dfs['dim_divisions']
dim_directorates    = dfs['dim_directorates']
dim_grades          = dfs['dim_grades']
dim_education       = dfs['dim_education']
dim_majors          = dfs['dim_majors']
dim_competency_pillars = dfs['dim_competency_pillars']
employees           = dfs['employees']
profiles_psych      = dfs['profiles_psych']
papi_scores         = dfs['papi_scores']
strengths           = dfs['strengths']
performance_yearly  = dfs['performance_yearly']
competencies_yearly = dfs['competencies_yearly']

# Make a copy of the original data for data cleaning
df_tv_tgv              = tv_tgv.copy()
df_dim_companies       = dim_companies.copy()
df_dim_areas           = dim_areas.copy()
df_dim_positions       = dim_positions.copy()
df_dim_departments     = dim_departments.copy()
df_dim_divisions       = dim_divisions.copy()
df_dim_directorates    = dim_directorates.copy()
df_dim_grades          = dim_grades.copy()
df_dim_education       = dim_education.copy()
df_dim_majors          = dim_majors.copy()
df_dim_competency_pillars = dim_competency_pillars.copy()
df_employees           = employees.copy()
df_profiles_psych      = profiles_psych.copy()
df_papi_scores         = papi_scores.copy()
df_strengths           = strengths.copy()
df_performance_yearly  = performance_yearly.copy()
df_competencies_yearly = competencies_yearly.copy()

df_tv_tgv.head(2)

df_dim_companies.head(2)

df_dim_areas.head(2)

df_dim_positions.head(2)

df_dim_departments.head(2)

df_dim_divisions.head(2)

df_dim_directorates.head(2)

df_dim_grades.head(2)

df_dim_education.head(2)

df_dim_majors.head(2)

df_dim_competency_pillars.head(2)

df_employees.head(2)

df_profiles_psych.head(2)

df_papi_scores.head(2)

df_strengths.head(2)

df_performance_yearly.head(2)

df_competencies_yearly.head(2)

"""## 2. Data Cleaning and Feature Engineering

### 2.1 Merge Data

#### 2.1.1 Merge Data For Employeee Table

*Since we want to analyze the employee table, we will join the name of id from employee table and and profile_psych table*
*Thats why we use left join.*
"""

# Merge the table with df_dim_companies
df_merged = df_employees.merge(df_dim_companies, how='left', left_on =['company_id'], right_on = ['company_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'company'}, inplace=True)
# Merge the table with df_dim_areas
df_merged = df_merged.merge(df_dim_areas, how='left', left_on =['area_id'], right_on = ['area_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'area'}, inplace=True)
# Merge the table with dim_positions
df_merged = df_merged.merge(dim_positions, how='left', left_on =['position_id'], right_on = ['position_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'position'}, inplace=True)
# Merge the table with df_dim_departments
df_merged = df_merged.merge(df_dim_departments, how='left', left_on =['department_id'], right_on = ['department_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'department'}, inplace=True)
# Merge the table with df_dim_divisions
df_merged = df_merged.merge(df_dim_divisions, how='left', left_on =['division_id'], right_on = ['division_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'division'}, inplace=True)
# Merge the table with df_dim_directorates
df_merged = df_merged.merge(df_dim_directorates, how='left', left_on =['directorate_id'], right_on = ['directorate_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'directorate'}, inplace=True)
# Merge the table with df_dim_grades
df_merged = df_merged.merge(df_dim_grades, how='left', left_on =['grade_id'], right_on = ['grade_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'grade'}, inplace=True)
# Merge the table with df_dim_education
df_merged = df_merged.merge(df_dim_education, how='left', left_on =['education_id'], right_on = ['education_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'education'}, inplace=True)
# Merge the table with df_dim_majors
df_merged = df_merged.merge(df_dim_majors, how='left', left_on =['major_id'], right_on = ['major_id'])
# Rename specific columns
df_merged.rename(columns={'name': 'major'}, inplace=True)
# Merge the table with df_profiles_psych
df_merged = df_merged.merge(df_profiles_psych, how='left', left_on =['employee_id'], right_on = ['employee_id'])
# Drop multiple columns
df_merged.drop(columns=['company_id', 'area_id', 'position_id', 'department_id', 'division_id', 'directorate_id', 'grade_id', 'education_id', 'major_id'], inplace=True)
# Show the head of the data
df_merged.head()

"""#### 2.1.2 Merge Data For competencies_yearly Table"""

# Merge the table
df_competencies_yearly = df_competencies_yearly.merge(df_dim_competency_pillars, how='left', left_on =['pillar_code'], right_on = ['pillar_code'])
df_competencies_yearly.head()

"""### 2.2  Data Cleaning Employee Table

#### 2.2.1 Understanding the Dataset
"""

df_employees_clean = df_merged.copy()

df_employees_clean.shape
print('Number of rows is: ', df_employees_clean.shape[0])
print('Number of columns is: ', df_employees_clean.shape[1])

df_employees_clean.info()

"""#### 2.2.2 Convert Data Type"""

# Change nip data type into string
df_employees_clean['nip'] = df_employees_clean['nip'].astype(str)
# Change years_of_service_months data type into integer
df_employees_clean['years_of_service_months'] = df_employees_clean['years_of_service_months'].astype(int)
# Change tiki data type into integer
df_employees_clean['tiki'] = df_employees_clean['tiki'].astype(int)

# Check the data type of date after converted
df_employees_clean.dtypes

"""#### 2.2.3 Checking for Unique Values and Handle Typo Data"""

# Check unique data in categorical columns
for column in df_employees_clean.select_dtypes(include=['object']).columns:
  num_unique = df_employees_clean[column].nunique()
  print(f"Number of unique '{column}' is {num_unique}")

"""*We will be checking for typos in specific columns of a dataset. The columns we will be checking are:*
* *company*
* *area*
* *position*
* *department*
* *division*
* *directorate*
* *grade*
* *education*
* *major*
* *disc*
* *disc_word*
* *mbti*


"""

# Count the values
list_obj = ['company','area', 'position', 'department', 'division', 'directorate', 'grade', 'education','major', 'disc', 'disc_word', 'mbti']

for i in list_obj:
    unique_data = df_employees_clean [i].value_counts()
    print(unique_data.to_markdown(), '\n\n')

"""*There are typos in the `mbti` columns and we will make it upper.*



"""

# Convert all MBTI values to uppercase
df_employees_clean['mbti'] = df_employees_clean['mbti'].str.upper()

# replace INFTJ with INFJ
df_employees_clean['mbti'] = df_employees_clean['mbti'].str.strip().str.upper().replace('INFTJ', 'INFJ')

# check if the fix worked
list_obj = ['mbti']

for i in list_obj:
    unique_data = df_employees_clean [i].value_counts()
    print(unique_data.to_markdown(), '\n\n')

"""#### 2.2.4 Missing Values"""

# Number of missing values in each column
df_employees_clean.isnull().sum()

# Clean the text before mapping
df_employees_clean['disc_word'] = (
  df_employees_clean['disc_word']
    .astype(str)
    .str.strip()                # remove spaces
    .str.replace('–', '-')      # fix en-dash vs hyphen
    .str.lower()                # standardize casing
)

# Make sure mapping keys are cleaned the same way
mapping = (
    df_employees_clean.dropna(subset=['disc', 'disc_word'])
      .assign(disc_word=lambda x: x['disc_word'].str.strip().str.replace('–', '-').str.lower())
      .set_index('disc_word')['disc']
      .to_dict()
)

# Then fill missing
df_employees_clean['disc'] = df_employees_clean['disc'].fillna(df_employees_clean['disc_word'].map(mapping))

# Number of missing values in each column
df_employees_clean.isnull().sum()

# Build a mapping dictionary to fill disc
mapping = df_employees_clean.dropna(subset=['disc', 'disc_word']).set_index('disc_word')['disc'].to_dict()
print(mapping)

df_employees_clean['iq'].describe()

# plotting
plt.figure(figsize=(8,5))
plt.hist(df_employees_clean['iq'].dropna(), bins=20, edgecolor='black')
plt.title('Distribution of IQ Scores')
plt.xlabel('IQ')
plt.ylabel('Frequency')
plt.show()

"""Since the distribution is not strongly skewed and both mean ≈ median,
we can safely fill missing IQ values using either mean or median, in this case mean
"""

# Fill missing value in iq column
df_employees_clean['iq'] = df_employees_clean['iq'].fillna(df_employees_clean['iq'].mean())

df_employees_clean['gtq'].describe()

# plotting
plt.figure(figsize=(8,5))
plt.hist(df_employees_clean['gtq'].dropna(), bins=20, edgecolor='black')
plt.title('Distribution of IQ Scores')
plt.xlabel('gtq')
plt.ylabel('Frequency')
plt.show()

"""Since the distribution is normally distributed (bell-shaped). that is, symmetric around the center with no strong skew, we should use the mean to fill missing values"""

# Fill missing value in iq column
df_employees_clean['gtq'] = df_employees_clean['gtq'].fillna(df_employees_clean['gtq'].mean())

# Impute missing values with "Unknown"
df_employees_clean['mbti'].fillna('Unknown',inplace = True)

df_employees_clean.isna().sum()

"""all null value has been treated

#### 2.2.5 Duplicated Data
"""

# We can check the duplicate pair by using parameter "keep=False"
df_employees_clean[df_employees_clean.duplicated(keep=False)]

"""*No duplicated data*

#### 2.2.6 Employees Cleaned Data
"""

# Create a copy after cleaning for other purpose, and its easier if there is something forgoten and needed to be clean later
employees_clean = df_employees_clean.copy()
employees_clean.head()

# For summary after data cleaning
employees_clean.info()

# Save to files for inspection and joining
employees_clean.to_csv("profiles_psych.csv", index=False)

print("Created profiles_psych.csv with joined employee table")

"""### 2.3 Data Cleaning competencies_yearly Table"""

df_competencies_yearly.head()

"""#### 2.3.1 Understanding the Dataset"""

df_competencies_yearly.shape
print('Number of rows is: ', df_competencies_yearly.shape[0])
print('Number of columns is: ', df_competencies_yearly.shape[1])

df_competencies_yearly.info()

df_competencies_yearly.head()

"""#### 2.3.2 Convert Data Type

##### a. Convert Data Type in Column `year`

*`year` can be changed into integer.*
"""

# Change id data type into string
df_competencies_yearly['year'] = df_competencies_yearly['year'].astype(int)

#Check the data type of id after converted
df_competencies_yearly.dtypes

"""#### 2.3.3 Checking for Unique Values and Handle Typo Data"""

# Check unique data in categorical columns
for column in df_competencies_yearly.select_dtypes(include=['object']).columns:
  num_unique = df_competencies_yearly[column].nunique()
  print(f"Number of unique '{column}' is {num_unique}")

"""*We will be checking for typos in specific columns of a dataset. The columns we will be checking are:*
* *pillar_code*
* *Cpillar_label*


*The other columns in the dataset are too diverse to check for typos*

*The diverse columns are:*
* *employee_id*
"""

# Count the values
list_obj = ['pillar_code','pillar_label']

for i in list_obj:
    unique_data = df_competencies_yearly[i].value_counts()
    print(unique_data.to_markdown(), '\n\n')

"""*There are no typos in the `Gender`, `Country`, and `Continent` columns because the values are either existing or valid.*

#### 2.3.4 Missing Values
"""

# Number of missing values in each column
df_competencies_yearly.isnull().sum()

"""* *score, 10791 row null*"""

# Check mean score per year before filling
print("Before filling missing values:")
print(df_competencies_yearly.groupby('year')['score'].mean())
print("\nNumber of missing values before:", df_competencies_yearly['score'].isna().sum())

# Fill missing scores with mean per year
df_competencies_yearly['score'] = df_competencies_yearly.groupby('year')['score'].transform(lambda x: x.fillna(x.mean()))

# Check mean score per year after filling
print("\nAfter filling missing values:")
print(df_competencies_yearly.groupby('year')['score'].mean())
print("\nNumber of missing values after:", df_competencies_yearly['score'].isna().sum())

# Display result
df_competencies_yearly.head()

# Confirm whether null data still exist
df_competencies_yearly.isnull().sum()

"""*All null's has been treated*

#### 2.3.5 Duplicated Data
"""

# Check the duplicated
df_competencies_yearly[df_competencies_yearly.duplicated()]

"""*No duplicated data*

#### 2.3.6 competencies_yearly Cleaned Data
"""

# Create a copy after cleaning for other purpose, and its easier if there is something forgoten and needed to be clean later
competencies_yearly_clean = df_competencies_yearly.copy()
competencies_yearly_clean.head()

# For summary after data cleaning
competencies_yearly_clean.info()

"""#### 2.3.7 Filter competencies_yearly for the latest data (2025) and turned pillar_code into a column"""

# Try to auto-detect expected columns
col_map = {
    "employee_id": next((c for c in competencies_yearly_clean.columns if c in {"employee_id"}), None),
    "year":        next((c for c in competencies_yearly_clean.columns if c in {"year"}), None),
    "pillar_code": next((c for c in competencies_yearly_clean.columns if c in {"pillar_code"}), None),
    "score":       next((c for c in competencies_yearly_clean.columns if c in {"score"}), None),
}

missing = [k for k,v in col_map.items() if v is None]
if missing:
    raise ValueError(f"Cannot find required columns in competencies_yearly: {missing}. "
                     f"Available: {list(competencies_yearly_clean.columns)}")

emp_col, year_col, pillar_col, score_col = col_map["employee_id"], col_map["year"], col_map["pillar_code"], col_map["score"]

# Clean values
competencies_yearly_clean[pillar_col] = competencies_yearly_clean[pillar_col].astype(str).str.strip().str.upper()
# ensure year is numeric for sorting
competencies_yearly_clean[year_col] = pd.to_numeric(competencies_yearly_clean[year_col], errors="coerce")
# keep only rows with known year and score
competencies_yearly_clean = competencies_yearly_clean.dropna(subset=[year_col, score_col])

# Keep latest year per employee × pillar
competencies_yearly_sorted = competencies_yearly_clean.sort_values([emp_col, pillar_col, year_col], ascending=[True, True, False])
competencies_yearly_latest = competencies_yearly_sorted.drop_duplicates(subset=[emp_col, pillar_col], keep="first")

# Pivot to wide columns: one column per pillar code
competencies_wide = competencies_yearly_latest.pivot(index=emp_col, columns=pillar_col, values=score_col)

# prefix column names for clarity
competencies_wide.columns = [f" score_comp_{c}" for c in competencies_wide.columns]

competencies_wide = competencies_wide.reset_index()

competencies_wide.head()

# Save outputs to inspect
competencies_wide.to_csv("competencies_latest_pivot.csv", index=False)

competencies_wide.info()

"""### 2.4 df_performance_yearly

#### 2.4.1 Understanding the Dataset
"""

# Make a copy for data cleaning
df_performance_yearly_clean = df_performance_yearly.copy()

df_performance_yearly_clean.shape
print('Number of rows is: ', df_performance_yearly_clean.shape[0])
print('Number of columns is: ', df_performance_yearly_clean.shape[1])

df_performance_yearly_clean.info()

df_performance_yearly_clean.head()

"""#### 2.4.2 Convert Data Type

##### a. Convert Data Type in Column `year`
"""

# Change id data type into string
df_performance_yearly_clean['year'] =df_performance_yearly_clean['year'].astype(int)

#Check the data type of id after converted
df_performance_yearly_clean.dtypes

"""#### 2.4.3 Checking for Unique Values and Handle Typo Data"""

# Check unique data in categorical columns
for column in df_performance_yearly_clean.select_dtypes(include=['object']).columns:
  num_unique = df_performance_yearly_clean[column].nunique()
  print(f"Number of unique '{column}' is {num_unique}")

"""*We will not be checking for typos in specific columns of a dataset beacuse the employee_id are too diverse to check for typos*

#### 2.4.4 Missing Values
"""

# Number of missing values in each column
df_performance_yearly_clean.isnull().sum()

#percentage of missing values of each column
round(df_performance_yearly_clean.isnull().sum()/len(df_performance_yearly_clean)*100,2)

"""*There are 1108 rows missing values detected in rating column*"""

# Check mean score per year before filling
print("Before filling missing values:")
print(df_performance_yearly_clean.groupby('year')['rating'].mean())
print("\nNumber of missing values before:", df_performance_yearly_clean['rating'].isna().sum())

# Fill missing scores with mean per year
df_performance_yearly_clean['rating'] = df_performance_yearly_clean.groupby('year')['rating'].transform(lambda x: x.fillna(x.mean()))

# Check mean score per year after filling
print("\nAfter filling missing values:")
print(df_performance_yearly_clean.groupby('year')['rating'].mean())
print("\nNumber of missing values after:", df_performance_yearly_clean['rating'].isna().sum())

# Display result
df_performance_yearly_clean.head()

"""#### 2.4.5 Duplicated Data"""

# Check the duplicated
df_performance_yearly_clean[df_performance_yearly_clean.duplicated()]

"""*No duplicated data*

#### 2.4.6 Performance-yearly Cleaned Data
"""

# Create a copy after cleaning for other purpose, and its easier if there is something forgoten and needed to be clean later
performance_yearly_clean = df_performance_yearly_clean.copy()
performance_yearly_clean.head()

# For summary after data cleaning
performance_yearly_clean.info()

"""#### 2.4.7 Filter performance_yearly Data from 2025 as the latest data

Create the High Performer Flag with rules

 High performer flag: is_top = 1 if rating == 5 else 0.
"""

# Create high performance flag
performance_yearly_clean['is_top'] = performance_yearly_clean['rating'].apply(lambda x: 1 if x == 5 else 0)

# keep only the latest per employee
if 'year' in performance_yearly_clean.columns:
   performance_yearly_clean = performance_yearly_clean.sort_values(['employee_id', 'year'], ascending=[True, False]) \
               .drop_duplicates('employee_id', keep='first')

# Check the distribution of the new flag
print(performance_yearly_clean['is_top'].value_counts())

# Save to verify
performance_yearly_clean.to_csv("performance_yearly_with_is_top.csv", index=False)
print("Added 'is_top' column and saved to performance_yearly_with_is_top.csv")

performance_yearly_clean.head()

"""### 2.5 Data Cleaning papi_scores Table

#### 2.4.1 Understanding the Dataset
"""

df_papi_scores.shape
print('Number of rows is: ', df_papi_scores.shape[0])
print('Number of columns is: ', df_papi_scores.shape[1])

df_papi_scores.info()

df_papi_scores.head()

"""#### 2.5.2 Convert Data Type"""

# Check the data type of df_papi_scores
df_papi_scores.dtypes

"""All data types are already correct

#### 2.5.3 Checking for Unique Values and Handle Typo Data
"""

# Check unique data in categorical columns
for column in df_papi_scores.select_dtypes(include=['object']).columns:
  num_unique = df_papi_scores[column].nunique()
  print(f"Number of unique '{column}' is {num_unique}")

"""*We will be checking for typos in specific columns of a dataset. The columns we will be checking are:*
* *scale_code*

*The other columns in the dataset are too diverse to check for typos*

*The diverse columns are:*
* *employee_id*
"""

# Count the values
list_obj = ['scale_code']

for i in list_obj:
    unique_data = df_papi_scores[i].value_counts()
    print(unique_data.to_markdown(), '\n\n')

"""*There are no typos in the `scale_code` columns because the values are either existing or valid.*

#### 2.5.4 Missing Values
"""

# Number of missing values in each column
df_papi_scores.isnull().sum()

#percentage of missing values of each column
round(df_papi_scores.isnull().sum()/len(df_papi_scores)*100,2)

"""* *`score`, 3203 row null which is 7.97% of the total data and we will fill it*

##### Handle Missing Values in Column `score`
"""

# plotting
plt.figure(figsize=(12,6))
sns.boxplot(data=df_papi_scores, x='scale_code', y='score')
plt.title('Score Distribution by Scale Code')
plt.xlabel('Scale Code')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.show()

"""Since there's no outlier we will fill it with mean per papi cetegory, for example papi_N score missing value will be filled with average (mean) score of papi_N score"""

# Check mean score per scale_code before filling
print("Before filling missing values:")
print(df_papi_scores.groupby('scale_code')['score'].mean())
print("\nNumber of missing values before:", df_papi_scores['score'].isna().sum())

# Fill missing scores with mean per scale_code
df_papi_scores['score'] = df_papi_scores.groupby('scale_code')['score'].transform(lambda x: x.fillna(x.mean()))

# Check mean score per scale_code after filling
print("\nAfter filling missing values:")
print(df_papi_scores.groupby('scale_code')['score'].mean())
print("\nNumber of missing values after:", df_papi_scores['score'].isna().sum())

# Display result
df_papi_scores.head()

"""*All null's has been treated*

#### 2.5.5 Duplicated Data
"""

# Check the duplicated
df_papi_scores[df_papi_scores.duplicated()]

"""*No duplicated data*

#### 2.5.6 papi_scores Cleaned Data
"""

# Create a copy after cleaning for other purpose, and its easier if there is something forgoten and needed to be clean later
papi_scores_clean = df_papi_scores.copy()
papi_scores_clean.head()

# For summary after data cleaning
papi_scores_clean.info()

"""#### 2.5.7 Turned Papi scale_code into a column"""

# Auto-detect key columns
col_map = {
    "employee_id": next((c for c in papi_scores_clean.columns if c in {"employee_id","employee id","id"}), None),
    "scale_code":  next((c for c in papi_scores_clean.columns if c in {"scale_code","scalecode","code","scale"}), None),
    "score":       next((c for c in papi_scores_clean.columns if c in {"score","value","rating","papi_score"}), None),
}

missing = [k for k,v in col_map.items() if v is None and k in {"employee_id","scale_code","score"}]
if missing:
    raise ValueError(f"Cannot find required columns in papi_scores: {missing}. "
                     f"Available: {list(papi_scores_clean.columns)}")

emp_col   = col_map["employee_id"]
code_col  = col_map["scale_code"]
score_col = col_map["score"]

# Clean values
papi_scores_clean[code_col] = papi_scores_clean[code_col].astype(str).str.strip().str.upper()
papi_scores_clean[score_col] = pd.to_numeric(papi_scores_clean[score_col], errors="coerce")

# drop rows without score or code
papi_scores_clean = papi_scores_clean.dropna(subset=[code_col, score_col])

# Pivot wide to one column per PAPI scale code
papi_wide = papi_scores_clean.pivot(index=emp_col, columns=code_col, values=score_col)

# Prefix for clarity
papi_wide.columns = [f"score_{c}" for c in papi_wide.columns]
papi_wide = papi_wide.reset_index()

papi_wide.head()

# Save to files for inspection and joining
papi_wide.to_csv("papi_latest_pivot.csv", index=False)

print("Created papi_latest_pivot.csv with columns like papi_D, papi_N, ...")

papi_wide.info()

"""### 2.6 Data Cleaning strengths table

#### 2.6.1 Understanding the Dataset
"""

df_strengths.shape
print('Number of rows is: ', df_strengths.shape[0])
print('Number of columns is: ', df_strengths.shape[1])

df_strengths.info()

df_strengths.head()

"""#### 2.6.2 Convert Data Type

##### Convert Data Type in Column `rank`

*`rank` will be change into integer.*
"""

# Change Srank data type into integer
df_strengths['rank'] = df_strengths['rank'].astype(int)

# Check the data type of rank after converted
df_strengths.dtypes

"""#### 2.6.3 Checking for Unique Values and Handle Typo Data"""

# Check unique data in categorical columns
for column in df_strengths.select_dtypes(include=['object']).columns:
  num_unique = df_strengths[column].nunique()
  print(f"Number of unique '{column}' is {num_unique}")

"""*We will be checking for typos in specific columns of a dataset. The columns we will be checking are:*
* *theme*
"""

# Count the values
list_obj = ['theme']

for i in list_obj:
    unique_data = df_strengths[i].value_counts()
    print(unique_data.to_markdown(), '\n\n')

"""*There are no typos in the `theme` columns because the values are either existing or valid.*

#### 2.6.4 Missing Values
"""

# Number of missing values in each column
df_strengths.isnull().sum()

# percentage of missing values of each column
round(df_strengths.isnull().sum()/len(df_strengths)*100,2)

"""*There are 2229 missing values detected on theme column*"""

df_strengths.loc[df_strengths['theme'].isnull()]

# Impute missing values with "Unknown"
df_strengths['theme'].fillna('Unknown',inplace = True)

df_strengths.loc[df_strengths['theme']=='Unknown']

# Confirm whether null data still exist
df_strengths.isnull().sum()

"""*All null's has been treated*

#### 2.6.5 Duplicated Data
"""

# Check the duplicated
df_strengths[df_strengths.duplicated()]

"""*No duplicated data*

#### 2.6.6 Strengths Cleaned Data
"""

# Create a copy after cleaning for other purpose, and its easier if there is something forgoten and needed to be clean later
strengths_clean = df_strengths.copy()
strengths_clean.head()

# For summary after data cleaning
strengths_clean.info()

"""#### 2.6.7 Turned Strengths rank into a column"""

# Auto-detect key columns
col_map = {
    "employee_id": next((c for c in strengths_clean.columns if c in {"employee_id","employee id","id"}), None),
    "rank":        next((c for c in strengths_clean.columns if c in {"rank","order","position","seq"}), None),
    "theme":       next((c for c in strengths_clean.columns if c in {"theme","strength","strength_theme","name"}), None),
}

missing = [k for k,v in col_map.items() if v is None and k in {"employee_id","rank","theme"}]
if missing:
    raise ValueError(f"Cannot find required columns in strengths: {missing}. "
                     f"Available: {list(strengths_clean.columns)}")

emp_col   = col_map["employee_id"]
rank_col  = col_map["rank"]
theme_col = col_map["theme"]

# Clean values
strengths_clean[theme_col] = strengths_clean[theme_col].astype(str).str.strip()
strengths_clean[rank_col]  = pd.to_numeric(strengths_clean[rank_col], errors="coerce")

# Drop rows missing essentials
need_cols = [emp_col, theme_col, rank_col]
strengths_clean = strengths_clean.dropna(subset=[emp_col, theme_col, rank_col])

# keep the best (lowest) rank per employee × theme, then one row per employee × rank
strengths_clean = (strengths_clean
         .sort_values([emp_col, theme_col, rank_col], ascending=[True, True, True])
         .drop_duplicates(subset=[emp_col, theme_col], keep="first")
         .sort_values([emp_col, rank_col], ascending=[True, True])
         .drop_duplicates(subset=[emp_col, rank_col], keep="first"))

# Limit to top K ranks beacuse we only need top 5
TOP_K = 5
strengths_clean = strengths_clean[strengths_clean[rank_col] <= TOP_K]

# Pivot wide: one column per rank with the theme as value
strengths_wide = strengths_clean.pivot(index=emp_col, columns=rank_col, values=theme_col)

# Build consistent column names like strength_rank_1 ... strength_rank_K
# Ensure ranks 1..TOP_K exist as columns even if missing for some employees
for r in range(1, TOP_K+1):
    if r not in strengths_wide.columns:
        strengths_wide[r] = pd.NA
strengths_wide = strengths_wide[sorted(strengths_wide.columns)]  # order by rank
strengths_wide.columns = [f"strength_rank_{int(c)}" for c in strengths_wide.columns]
strengths_wide = strengths_wide.reset_index()

strengths_wide.tail()

# Save outputs
strengths_wide.to_csv("strengths.csv", index=False)

print("strengths.csv created with columns strength_rank_1 ... strength_rank_")

"""### 2.7 Merge sll the table

"""

# Standardize key name
key = "employee_id"
for df in [employees_clean, performance_yearly_clean, competencies_wide, papi_wide, strengths_wide]:
    if key not in df.columns:
        candidates = [c for c in df.columns if "id" in c]
        if candidates:
            df.rename(columns={candidates[0]: key}, inplace=True)

# Perform successive left joins
df_model = (
    employees_clean
    .merge(performance_yearly_clean, on=key, how="left", suffixes=("", "_perf"))
    .merge(competencies_wide, on=key, how="left")
    .merge(papi_wide, on=key, how="left")
    .merge(strengths_wide, on=key, how="left")
)

# Quick sanity check
df_model.head(3)

# Save the file
df_model.to_csv("df_model_full.csv", index=False)
print("Saved merged dataset to df_model_full.csv")

"""## 3. Exploratory Data Analysis"""

df_model.info()

df_model.shape
print('Number of rows is: ', df_model.shape[0])
print('Number of columns is: ', df_model.shape[1])

"""### 3.1 Descriptive Statistics

#### 3.1.1 Numerical Variables
"""

numerical = df_model.select_dtypes(include = ['number'])

desc_num = numerical.describe()
desc_num.loc['kurtosis'] = numerical.kurt() # Show kurtosis statistic
desc_num.loc['skewness'] = numerical.skew() # Show Skewness statistic
desc_num.loc['variance'] = numerical.var() # Show variance statistic
desc_num.round(2)

"""#### 3.1.2 Categorical Variables"""

categorical = df_model.select_dtypes(exclude=['number'])
categorical.describe()

"""### 3.2 Distribution of Employee by Rating 5

*How many employee has 5 star?*
"""

vc = df_model['is_top'].value_counts().sort_index()
plt.figure(figsize=(4,4)); plt.bar(['Non-top','Top'], vc.values, color='skyblue'); plt.title('Top Performace Employee Comparison'); plt.tight_layout()

# Value counts
vc = df_model['is_top'].value_counts().sort_index()

# Pie chart
plt.figure(figsize=(5,5))
plt.pie(
    vc.values,
    labels=['Non-top', 'Top'],
    autopct='%1.1f%%',
    startangle=90,
    colors=['skyblue', 'orange'],
    explode=(0, 0.05)  # separate the 'Top' slice slightly
)
plt.title('Top Performace Employee Comparison')
plt.tight_layout()
plt.show()

df_is_top = df_model['is_top'].value_counts()
df_is_top.reset_index()

"""The dataset is imbalanced with
1. *Top Performance: There are 168 (8,4%) top employee with rating = 5*

2. *Non Top Performance: There are 1,842 (91,6%) non-top employee with rating < 5*

### 3.3 Rating distribution

purpose : context for the 5-point scale.
"""

rating_dis = df_model['rating'].dropna().astype(int).value_counts().sort_index()
plt.figure(figsize=(5,4)); plt.bar(rating_dis.index.astype(str), rating_dis.values, color='skyblue')
plt.title('Rating distribution'); plt.xlabel('Rating'); plt.tight_layout();

"""Ratings are skewed toward middle. This sets the base rate of top performers and helps calibrate expectations for lift.

### 3.4 Competency differences by target

purpose : show which competency pillars differ between top vs others.
"""

# Ensure lowercase column names
df_model.columns = df_model.columns.str.lower().str.strip()

# Capture competency columns (case-insensitive)
comp_cols = [c for c in df_model.columns if c.startswith('score_comp_')]

if not comp_cols:
    print("No columns found starting with 'score_comp_'. Check column names.")
else:
    # Convert to numeric safely
    df_comp = df_model[comp_cols].apply(pd.to_numeric, errors='coerce')

    # Compute mean difference (top - non-top)
    means_top = df_comp[df_model.is_top == 1].mean()
    means_non = df_comp[df_model.is_top == 0].mean()
    diff = (means_top - means_non).dropna().sort_values()

    if diff.empty:
        print("All competency values are NaN or non-numeric.")
    else:
        plt.figure(figsize=(10, 6))
        plt.barh(diff.index, diff.values, color='skyblue')
        plt.title("Competency mean difference (Top minus Non-top)")
        plt.xlabel("Mean difference")
        plt.ylabel("Competency Pillar")
        plt.grid(axis='x', linestyle='--', alpha=0.6)
        plt.tight_layout()
        plt.show()   # ensures the chart appears

"""### 3.5 PAPI profile comparison

purpose : show personality scale gaps that matter.
"""

# Standardize column names
df_model.columns = df_model.columns.str.lower().str.strip()

# Detect all PAPI columns (works for 'papi_', 'score_papi_', etc.)
papi_cols = [c for c in df_model.columns if 'papi' in c.lower()]

if not papi_cols:
    print("No PAPI columns found. Check column names.")
else:
    df_papi = df_model[papi_cols].apply(pd.to_numeric, errors='coerce')

    # Compute means for top vs non-top
    mt = df_papi[df_model.is_top == 1].mean()
    mn = df_papi[df_model.is_top == 0].mean()

    # Top 15 differences by absolute gap
    order = (mt - mn).abs().sort_values(ascending=False).index[:15]

    plt.figure(figsize=(9, 6))
    plt.plot(range(len(order)), mt[order].values, marker='o', label='Top', color='skyblue')
    plt.plot(range(len(order)), mn[order].values, marker='x', label='Non-top', color='orange')
    plt.xticks(range(len(order)), [c.replace('papi_', '').upper() for c in order],
               rotation=45, ha='right')
    plt.title('PAPI profile — Top vs Non-top')
    plt.xlabel('PAPI Scales')
    plt.ylabel('Average Score')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

"""### 3.6 Strengths top-k prevalence

Purpose : to understand which themes appear more in top performers.
"""

# Detect all strength rank columns (case-insensitive)
rank_cols = [c for c in df_model.columns if c.lower().startswith('strength_rank_')]

def count_strengths(sub):
    """Count theme frequencies across all rank columns for a given subset."""
    s = pd.Series(dtype=int)
    for c in rank_cols:
        vals = sub[c].astype(str).str.strip().str.title()
        # Exclude 'Unknown'
        vals = vals[~vals.isin(["Unknown"])]
        s = s.add(vals.value_counts(), fill_value=0)
    return s.sort_values(ascending=False)

# Count themes for each group
top_counts = count_strengths(df_model[df_model.is_top == 1])
non_counts = count_strengths(df_model[df_model.is_top == 0])

# Select top 15 most common themes overall
themes = top_counts.add(non_counts, fill_value=0).sort_values(ascending=False).head(15).index

# Plot
plt.figure(figsize=(10, 5))
x = range(len(themes))

plt.bar(x, non_counts[themes].values, color='orange', alpha=0.7, label='Non-top')
plt.bar(x, top_counts[themes].values, color='skyblue', alpha=0.8, label='Top')

plt.xticks(x, themes, rotation=45, ha='right')
plt.title('Strengths Prevalence (Top vs Non-top Performers)')
plt.ylabel('Count')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

"""### 3.7 Effect size bar for numeric features

purpose: quick ranking of strongest numeric differences.
"""

df_model.head(2)

num_cols = [c for c in df_model.select_dtypes('number').columns if c not in {'employee_id','is_top'}]
def cohens_d(x, y):
    x = pd.to_numeric(x, errors='coerce').dropna(); y = pd.to_numeric(y, errors='coerce').dropna()
    if len(x)<3 or len(y)<3: return np.nan
    vx, vy = x.var(ddof=1), y.var(ddof=1)
    pooled = ((len(x)-1)*vx + (len(y)-1)*vy) / (len(x)+len(y)-2) if (len(x)+len(y)-2)>0 else np.nan
    return (x.mean()-y.mean())/np.sqrt(pooled) if pooled>0 else np.nan
rows = []
for c in num_cols:
    rows.append((c, cohens_d(df_model[df_model.is_top==1][c], df_model[df_model.is_top==0][c])))
es = pd.DataFrame(rows, columns=['feature','d']).dropna().set_index('feature').iloc[:,0].abs().sort_values(ascending=False).head(20)
plt.figure(figsize=(8,6)); plt.barh(es.index[::-1], es.values[::-1], color='skyblue'); plt.title("Top numeric effect sizes"); plt.tight_layout()

"""### 3.8 Cohort pass rate by grade or tenure

Purpose: control for seniority or time in role.
"""

# By grade
grade_order = df_model['grade'].dropna().astype(str).unique()
gp = df_model.groupby('grade')['is_top'].mean().reindex(sorted(grade_order))
plt.figure(figsize=(7,4)); plt.bar(gp.index.astype(str), (gp.values*100),  color='skyblue')
plt.title('Top rate by grade (%)'); plt.tight_layout()

# By tenure buckets
bins = [0,12,24,48,96,9999]; labels = ['<=12m','13-24m','25-48m','49-96m','>96m']
ten = pd.cut(df_model['years_of_service_months'], bins=bins, labels=labels, include_lowest=True)
tp = df_model.groupby(ten)['is_top'].mean().reindex(labels)
plt.figure(figsize=(7,4)); plt.bar(tp.index.astype(str), (tp.values*100), color='skyblue')
plt.title('Top rate by tenure bucket (%)'); plt.tight_layout()

"""### 3.9 Competency boxplots by target

Purpose: show spread and overlap for a few key pillars.
"""

keys = diff.tail(6).index if 'diff' in globals() else comp_cols[:6]
for c in keys:
    plt.figure(figsize=(5,3))
    data0 = df_model[df_model.is_top==0][c].dropna()
    data1 = df_model[df_model.is_top==1][c].dropna()
    plt.boxplot([data0, data1], labels=['Non-top','Top'])
    plt.title(f'{c} distribution'); plt.tight_layout()

"""### 3.10 Pair plot substitute for top features

purpose: simple relationship view vs target
"""

top4 = es.index[:4].tolist() if 'es' in globals() else num_cols[:4]
for c in top4:
    plt.figure(figsize=(5,3))
    plt.scatter(df_model[df_model.is_top==0][c], df_model[df_model.is_top==0]['rating'], s=10, label='Non-top')
    plt.scatter(df_model[df_model.is_top==1][c], df_model[df_model.is_top==1]['rating'], s=10, label='Top')
    plt.xlabel(c); plt.ylabel('Rating'); plt.legend(); plt.tight_layout()

"""3.11 The average IQ, GTQ, TIKI, and Strength Match for top vs non-top performers."""

# Identify key columns
iq_col = next((c for c in df_model.columns if c.startswith('iq')), None)
gtq_cols = [c for c in df_model.columns if c.startswith('gtq')]
tiki_cols = [c for c in df_model.columns if c.startswith('tiki')]

# Compute mean scores for GTQ and TIKI
if gtq_cols:
    df_model['gtq_mean'] = df_model[gtq_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)
if tiki_cols:
    df_model['tiki_mean'] = df_model[tiki_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# Clean MBTI values (exclude Unknown)
if 'mbti' in df_model.columns:
    df_model['mbti_clean'] = (
        df_model['mbti']
        .astype(str)
        .str.strip()
        .replace(['', 'nan', 'none', 'unknown', 'null', 'UNK', 'N/A'], np.nan)
        .str.upper()
    )
else:
    df_model['mbti_clean'] = np.nan

#  Compute group means for IQ, GTQ, TIKI
summary = {
    'IQ': df_model.groupby('is_top')[iq_col].mean() if iq_col else pd.Series([np.nan, np.nan]),
    'GTQ': df_model.groupby('is_top')['gtq_mean'].mean() if 'gtq_mean' in df_model else pd.Series([np.nan, np.nan]),
    'TIKI': df_model.groupby('is_top')['tiki_mean'].mean() if 'tiki_mean' in df_model else pd.Series([np.nan, np.nan]),
}

# MBTI distribution comparison (Top 5 types)
# Exclude rows where mbti_clean is NaN (Unknown or invalid)
df_valid_mbti = df_model[df_model['mbti_clean'].notna()]

if not df_valid_mbti.empty:
    top_mbti = df_valid_mbti[df_valid_mbti.is_top==1]['mbti_clean'].value_counts(normalize=True) * 100
    non_mbti = df_valid_mbti[df_valid_mbti.is_top==0]['mbti_clean'].value_counts(normalize=True) * 100
    common_mbti = top_mbti.add(non_mbti, fill_value=0).sort_values(ascending=False).head(5).index
else:
    common_mbti = []

#  Main chart: IQ, GTQ, TIKI
labels = ['IQ', 'GTQ', 'TIKI']
top_means = [summary[k][1] for k in labels]
non_means = [summary[k][0] for k in labels]

x = np.arange(len(labels))
width = 0.35

plt.figure(figsize=(8,5))
plt.bar(x - width/2, non_means, width, color='orange', alpha=0.7, label='Non-top')
plt.bar(x + width/2, top_means, width, color='skyblue', alpha=0.8, label='Top')
plt.xticks(x, labels)
plt.ylabel('Average Score')
plt.title('IQ, GTQ, and TIKI (Top vs Non-top Performers)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

""" Top performers show higher IQ and GTQ averages, while TIKI follows a similar trend. It means that ognitive metrics appear linked to performance."""

# MBTI distribution chart
if 'mbti' in df_model.columns:
    # Clean MBTI text
    df_model['mbti_clean'] = (
        df_model['mbti']
        .astype(str)
        .str.strip()
        .str.upper()
        .replace({
            '': np.nan,
            'UNKNOWN': np.nan,
        })
    )

# Drop invalid rows
df_valid_mbti = df_model[df_model['mbti_clean'].notna()]

if not df_valid_mbti.empty:
    # Calculate MBTI distributions
    top_mbti = df_valid_mbti[df_valid_mbti.is_top==1]['mbti_clean'].value_counts(normalize=True) * 100
    non_mbti = df_valid_mbti[df_valid_mbti.is_top==0]['mbti_clean'].value_counts(normalize=True) * 100

    # Determine most common types overall
    common_mbti = top_mbti.add(non_mbti, fill_value=0).sort_values(ascending=False).head(5).index

    # Plot
    plt.figure(figsize=(8,5))
    width = 0.35
    plt.bar(np.arange(len(common_mbti)) - width/2, non_mbti[common_mbti].values, width,
            color='orange', alpha=0.7, label='Non-top')
    plt.bar(np.arange(len(common_mbti)) + width/2, top_mbti[common_mbti].values, width,
            color='skyblue', alpha=0.8, label='Top')

    plt.xticks(range(len(common_mbti)), common_mbti)
    plt.ylabel('Percentage (%)')
    plt.title('Top 5 MBTI Types (Top vs Non-top)')
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()
else:
    print("No valid MBTI data after excluding all forms of 'Unknown'")

"""purpose: Shows the average IQ, GTQ, TIKI, and Strength Match for top vs non-top performers

### 3.12 Directorates heatmap style bar

Purpose: show pockets of excellence.
"""

top_directorate = (df_model.groupby('directorate')['is_top'].mean()*100).sort_values()
plt.figure(figsize=(8,5)); plt.barh(top_directorate.index.astype(str), top_directorate.values, color='skyblue'); plt.title('Top rate by directorate (%)'); plt.tight_layout()

"""### 3.13 Candidate vs benchmark radar-style bars

Purpose: explain individual fit to the benchmark without ML.
"""

# pick a candidate_id and a feature list
candidate_id = df_model['employee_id'].iloc[0]
feat = comp_cols[:10] + papi_cols[:20] if len(comp_cols)>=4 and len(papi_cols)>=4 else (comp_cols[:6])
bench = df_model[df_model.is_top==1][feat].mean()
cand = df_model[df_model.employee_id==candidate_id][feat].iloc[0]
x = range(len(feat))
plt.figure(figsize=(9,4))
plt.bar(x, bench.values, alpha=0.6, label='Benchmark (Top avg)', color='skyblue')
plt.plot(x, bench.values, color='orange', linestyle='--')
plt.bar(x, cand.values, alpha=0.8, label=f'Employee {candidate_id}', color='orange')
plt.xticks(x, feat, rotation=45, ha='right'); plt.legend(); plt.title('Candidate vs benchmark'); plt.tight_layout()

"""3.14 Radar Style Bar"""

# Make sure all column names are lowercase
df_model.columns = df_model.columns.str.lower().str.strip()

# prepare numeric psychological variables
# Detect IQ, GTQ, and TIKI columns
iq_col = next((c for c in df_model.columns if c.startswith('iq')), None)
gtq_cols = [c for c in df_model.columns if c.startswith('gtq')]
tiki_cols = [c for c in df_model.columns if c.startswith('tiki')]

# Compute averages
if gtq_cols:
    df_model['gtq_mean'] = df_model[gtq_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)
if tiki_cols:
    df_model['tiki_mean'] = df_model[tiki_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# compute benchmark (top performers)
bench = {}
if iq_col: bench['iq'] = df_model.loc[df_model.is_top==1, iq_col].mean()
if gtq_cols: bench['gtq'] = df_model.loc[df_model.is_top==1, 'gtq_mean'].mean()
if tiki_cols: bench['tiki'] = df_model.loc[df_model.is_top==1, 'tiki_mean'].mean()

# For MBTI, we’ll find the most common type among top performers
if 'mbti' in df_model.columns:
    bench['mbti'] = df_model.loc[df_model.is_top==1, 'mbti'].mode().iloc[0]

# Exclude 'Unknown' when taking the MBTI mode
if 'mbti' in df_model.columns:
    valid_mbti = df_model.loc[df_model['mbti'].notna() & (df_model['mbti'].str.lower() != 'unknown'), 'mbti']
    if not valid_mbti.empty:
        bench['mbti'] = valid_mbti.mode().iloc[0]
    else:
        bench['mbti'] = None

# Benchmark top strengths (most frequent top 5)
rank_cols = [c for c in df_model.columns if c.startswith('strength_rank_')]
strengths_top = []
for c in rank_cols:
    vals = df_model.loc[df_model.is_top==1, c].dropna().astype(str).str.strip()
    vals = vals[~vals.str.lower().isin(['unknown'])]
    strengths_top.extend(vals)
top_strengths_benchmark = pd.Series(strengths_top).value_counts().head(5).index.tolist()

# choose candidate
candidate_id = df_model['employee_id'].iloc[0]
cand = df_model[df_model['employee_id']==candidate_id].iloc[0]

# Compute candidate values
vals = {}
if iq_col: vals['iq'] = cand[iq_col]
if gtq_cols: vals['gtq'] = cand['gtq_mean']
if tiki_cols: vals['tiki'] = cand['tiki_mean']
if 'mbti' in df_model.columns:
    vals['mbti'] = 1 if cand['mbti'] == bench['mbti'] else 0  # 1 = same, 0 = different

# Strength match (exclude 'Unknown')
cand_strengths = [str(cand[c]).strip() for c in rank_cols if pd.notna(cand[c])]
cand_strengths = [s for s in cand_strengths if s.lower() not in ['unknown']]
overlap = len(set(cand_strengths) & set(top_strengths_benchmark))
vals['strength_match'] = overlap / max(len(top_strengths_benchmark), 1)

# prepare data for chart
categories = ['iq', 'gtq', 'tiki', 'strength_match']
cand_values = [vals.get(k, np.nan) for k in categories]
bench_values = [bench.get(k, np.nan) if k != 'strength_match' else 1.0 for k in categories]

# Normalize if different scales
cand_norm = np.array(cand_values, dtype=float)
bench_norm = np.array(bench_values, dtype=float)
if not np.all(np.isnan(cand_norm)):
    scale = np.nanmax(np.concatenate([cand_norm, bench_norm]))
    if scale > 0:
        cand_norm /= scale
        bench_norm /= scale

# plot radar-style bars
x = np.arange(len(categories))
width = 0.35

plt.figure(figsize=(8,5))
plt.bar(x - width/2, bench_norm, width, label='Benchmark (Top avg)', color='orange', alpha=0.7)
plt.bar(x + width/2, cand_norm, width, label=f'Candidate {candidate_id}', color='skyblue', alpha=0.8)

plt.xticks(x, [c.upper() for c in categories])
plt.ylabel('Relative Score (normalized)')
plt.title('Candidate vs Benchmark (Psychological & Strength Profile)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

print("Benchmark top 5 strengths:", top_strengths_benchmark)
print("Candidate strengths:", cand_strengths)
print(f"Strength match: {vals['strength_match']*100:.1f}%")

df_model.head(2)

"""### 3.14 Psychometric Trait

Purpose : reveal whether behavioral or cognitive patterns actually contribute to higher performance.
"""

# Identify columns
psy_cols = [c for c in ['pauli','faxtor'] if c in df_model.columns]

# Convert to numeric where possible
df_psy = df_model.copy()
for c in psy_cols:
    df_psy[c] = pd.to_numeric(df_psy[c], errors='coerce')

# Compute group means
means = df_psy.groupby('is_top')[psy_cols].mean().T
means.columns = ['Non-top','Top']
means['Gap'] = means['Top'] - means['Non-top']
display(means)

# Plot
plt.figure(figsize=(7,4))
x = range(len(psy_cols))
width = 0.35
plt.bar([i - width/2 for i in x], means['Non-top'], width, color='orange', alpha=0.7, label='Non-top')
plt.bar([i + width/2 for i in x], means['Top'], width, color='skyblue', alpha=0.8, label='Top')
plt.xticks(x, [c.upper() for c in psy_cols])
plt.ylabel('Average Score')
plt.title('Pauli and Faxtor (Top vs Non-top)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

if 'disc' in df_model.columns:
    disc_clean = (df_model['disc']
                  .astype(str)
                  .str.strip()
                  .str.upper()
                  .replace(['UNKNOWN'], pd.NA))
    df_model['disc_clean'] = disc_clean
    valid = df_model[df_model['disc_clean'].notna()]

    if not valid.empty:
        top_disc = valid[valid.is_top==1]['disc_clean'].value_counts(normalize=True)*100
        non_disc = valid[valid.is_top==0]['disc_clean'].value_counts(normalize=True)*100
        common = top_disc.add(non_disc, fill_value=0).sort_values(ascending=False).index

        plt.figure(figsize=(8,5))
        plt.bar(range(len(common)), non_disc[common].values, width=0.35, color='orange', alpha=0.7, label='Non-top')
        plt.bar([i + 0.35 for i in range(len(common))], top_disc[common].values, width=0.35,
                color='skyblue', alpha=0.8, label='Top')
        plt.xticks(range(len(common)), common)
        plt.ylabel('Percentage (%)')
        plt.title('DISC Type Distribution (Top vs Non-top)')
        plt.legend()
        plt.grid(axis='y', linestyle='--', alpha=0.5)
        plt.tight_layout()
        plt.show()

"""### 3.15 Benchmark Baseline"""

# Create a copy with renamed columns for papi scores
rename_map = {c: c.replace("score_papi_", "papi_") for c in df_model.columns if c.startswith("score_papi_")}
df_norm = df_model.rename(columns=rename_map)

# Define numeric TVs (now matching normalized names)
numeric_tvs = [
    "gtq", "iq", "tiki", "pauli",
    "papi_a", "papi_b", "papi_c", "papi_d", "papi_e",
    "papi_g", "papi_k", "papi_l", "papi_n", "papi_o",
    "papi_p", "papi_s", "papi_t"
]

#  Filter benchmark group (Top Performers)
benchmark_df = df_norm[df_norm["is_top"] == 1]

# Compute medians for each numeric TV
baseline_medians = (
    benchmark_df[numeric_tvs]
    .apply(pd.to_numeric, errors="coerce")
    .median()
    .reset_index()
    .round(0)
)
baseline_medians.columns = ["tv_name", "benchmark_median"]

# Define TGV mapping (matching normalized names)
tgv_map = {
    "gtq": "Cognitive Complexity & Problem-Solving",
    "iq": "Cognitive Complexity & Problem-Solving",
    "tiki": "Cognitive Complexity & Problem-Solving",
    "pauli": "Motivation & Drive",
    "papi_a": "Motivation & Drive",
    "papi_g": "Motivation & Drive",
    "papi_n": "Motivation & Drive",
    "papi_l": "Leadership & Influence",
    "papi_p": "Leadership & Influence",
    "papi_k": "Leadership & Influence",
    "papi_s": "Social Orientation & Collaboration",
    "papi_b": "Social Orientation & Collaboration",
    "papi_o": "Social Orientation & Collaboration",
    "papi_c": "Conscientiousness & Reliability",
    "papi_d": "Conscientiousness & Reliability",
    "papi_e": "Adaptability & Stress Tolerance",
    "papi_t": "Adaptability & Stress Tolerance",
}

# Add the TGV mapping
baseline_medians["tgv_name"] = baseline_medians["tv_name"].map(tgv_map)

# Display the cleaned table
print(baseline_medians)

# Export for reference / SQL import
baseline_medians.to_csv("benchmark_baselines.csv", index=False)

"""## 4. Insights

*   Top performers stand out through a balanced combination of cognitive agility,social intelligence, and consistent behavioral style. They score higher on GTQ (reflecting sustained cognitive engagement), with supporting strength in IQ and TIKI, showing that learning capacity and time management underpin strong performance.
*   Their competency advantage is clearest in Social Empathy & Awareness (SEA), the single most differentiating skill. Followed by Value Creation for Users (VCU), Synergy & Team Orientation (STO), Commercial Savvy & Impact (CSI), and Curiosity & Experimentation (CEX), all of which point to collaboration and business acumen.
*   Psychometric results such as Pauli and Faxtor reinforce this pattern with modest but consistent gains, indicating that focus and cognitive flexibility further enable success.
*  On the behavioral side, DISC profiles SI, SD, and IS dominate among top performers, revealing a blend of steadiness, influence, and Dominant, which means people who are patient, cooperative, persuasive, and direct when needed.
Their MBTI patterns show stronger Perceiving and Intuitive preferences, aligning with adaptability and strategic thinking.
*   The benchmark strengths (Futuristic, Restorative, Self-Assurance, Intellection, Activator) reflect a forward-looking, self-driven mindset. Collectively, this profile defines a performer who is emotionally intelligent, cognitively engaged, future-oriented, and relationally stable.
"""